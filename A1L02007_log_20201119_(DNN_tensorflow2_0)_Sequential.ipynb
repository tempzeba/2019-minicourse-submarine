{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1L02007_log_20201119 (DNN-tensorflow2.0)-Sequential.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKHw+9JJXZKqVx4LMr2KbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tempzeba/2019-minicourse-submarine/blob/master/A1L02007_log_20201119_(DNN_tensorflow2_0)_Sequential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y0w5q6AUmMa"
      },
      "source": [
        "## 0) 程式說明\n",
        "### 目的：A1L02007 IOT異常分析\n",
        "### 運行環境："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhSdZF2NW8Xs"
      },
      "source": [
        "import sys\n",
        "print(\"python版本:%s\"% sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIVId25Van3a"
      },
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5wddLSban3o"
      },
      "source": [
        "## 1) 載入資料集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA-bjU2pan3p"
      },
      "source": [
        "# df_train = pd.read_csv('./data/Iris.csv')\n",
        "df_train = pd.read_csv('https://github.com/tempzeba/iot_test_public/releases/download/iot_data/A1L02007_log_01.csv', sep='\\t')\n",
        "df_train = df_train.drop(labels=[r'收集日期', \n",
        "r'抽樣秒', \n",
        "r'位址區段E32768紀錄時間', \n",
        "r'位址區段G32768紀錄時間', \n",
        "r'位址區段G12288紀錄時間',\n",
        "r'機台編號',\n",
        "r'目前生產配方'\n",
        ",\n",
        " '壓力檢測啟動',\n",
        " 'Cycle_Time',\n",
        " '第一穴預吹延遲',\n",
        " '第二穴預吹延遲',\n",
        " '第三穴預吹延遲',\n",
        " '第四穴預吹延遲',\n",
        " '第五穴預吹延遲',\n",
        " '第六穴預吹延遲',\n",
        " '第七穴預吹延遲',\n",
        " '第八穴預吹延遲',\n",
        " '第九穴預吹延遲',\n",
        " '第十穴預吹延遲',\n",
        " '第一穴高壓延遲',\n",
        " '第二穴高壓延遲',\n",
        " '第三穴高壓延遲',\n",
        " '第四穴高壓延遲',\n",
        " '第五穴高壓延遲',\n",
        " '第六穴高壓延遲',\n",
        " '第七穴高壓延遲',\n",
        " '第八穴高壓延遲',\n",
        " '第九穴高壓延遲',\n",
        " '第十穴高壓延遲',\n",
        " '第一穴吹高壓時間',\n",
        " '第二穴吹高壓時間',\n",
        " '第三穴吹高壓時間',\n",
        " '第四穴吹高壓時間',\n",
        " '第五穴吹高壓時間',\n",
        " '第六穴吹高壓時間',\n",
        " '第七穴吹高壓時間',\n",
        " '第八穴吹高壓時間',\n",
        " '第九穴吹高壓時間',\n",
        " '第十穴吹高壓時間',\n",
        " '第一穴排氣時間',\n",
        " '第二穴排氣時間',\n",
        " '第三穴排氣時間',\n",
        " '第四穴排氣時間',\n",
        " '第五穴排氣時間',\n",
        " '第六穴排氣時間',\n",
        " '第七穴排氣時間',\n",
        " '第八穴排氣時間',\n",
        " '第九穴排氣時間',\n",
        " '第十穴排氣時間',\n",
        " '第九層燈管加熱%',\n",
        " '第十層燈管加熱%',\n",
        " '產能',\n",
        " '吹氣鼓風機轉速',\n",
        " '吸氣鼓風機轉速',\n",
        " '自轉馬達轉速',\n",
        " '整列滾筒調速',\n",
        " '牙口鼓風機轉速',\n",
        " '入胚啟動位置',\n",
        " '入胚啟動延時',\n",
        " '震動馬達震動時間',\n",
        " '震動馬達震動循環時間',\n",
        " '第九穴壓力檢測',\n",
        " '第十穴壓力檢測',\n",
        "#  '破瓶數增加',\n",
        " '入胚啟動',\n",
        " '加熱啟動',\n",
        " '吹瓶啟動',\n",
        " '機台異常或無異常',\n",
        " '自動或手動',\n",
        " '總生產數量',\n",
        " '機台是否正常(0~59s)'\n",
        "\n",
        "],axis=1) \n",
        "\n",
        "# 調整資料集\n",
        "# 1. 只取有異常部分\n",
        "# 2. 計算有異常資料筆數\n",
        "# 3. 從無異常資料隨機抽出k筆資料與異常資料合併作為投入模型的資料集。k=異常資料筆數\n",
        "mask = df_train[r'異常代碼'] > 0\n",
        "target_count = mask.sum()\n",
        "df_train_noerr = df_train[df_train[r'異常代碼'] == 0].sample(n=target_count,random_state=123,axis=0)\n",
        "df_train = df_train[mask]\n",
        "df_train = df_train.append(df_train_noerr)\n",
        "df_train.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1M7a4iian3u"
      },
      "source": [
        "## 2) 手動編碼\n",
        "處理名目資料 (Nominal variables) - 資料前處理\n",
        "依據特徵資料的特性，可以選擇手動編碼或自動編碼。\n",
        "\n",
        "### 使用編碼時機?\n",
        "進行深度學習時，神經網路只能處理數值資料。因此我們需要將所有非數字型態的特徵進行轉換。\n",
        "\n",
        "### 註：\n",
        "略。(IOT原始資料已經一次前處理)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txpEEPgAan36"
      },
      "source": [
        "## 3) 檢查缺失值\n",
        "使用 numpy 所提供的函式來檢查是否有 NA 缺失值，假設有缺失值使用dropna()來移除。使用的時機在於當只有少量的缺失值適用，若遇到有大量缺失值的情況，或是本身的資料量就很少的情況下建議可以透過機器學習的方法補值來預測缺失值。\n",
        "### 註：\n",
        "略。(IOT原始資料已經一次前處理)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNBeqTcUan4A"
      },
      "source": [
        "## 4) 將輸出特徵做one-hot encoding\n",
        "如何對欄位class做one-hot encoding？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQFh4fj0an4C"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(df_train[r'異常代碼'])\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OclZM7Wan4I"
      },
      "source": [
        "## 5) 切割訓練集與測試集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSw4NVQLwdvU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDEttMPan4J"
      },
      "source": [
        "pd.set_option('max_row',300)\n",
        "# X=df_train.drop(labels=['Class'],axis=1)\n",
        "X=df_train.drop(columns=[r'異常代碼'])\n",
        "# X.isna().sum().sum()\n",
        "# X\n",
        "mask = X.describe().transpose()['std']==0\n",
        "X.describe().transpose()[mask].index.tolist()\n",
        "# X.describe().transpose().columns.tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKl7fICTan4Q"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y , test_size=.3 , random_state=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ovl96zhan4T"
      },
      "source": [
        "print('訓練資料: ',X_train.shape)\n",
        "print('測試資料: ',X_test.shape)\n",
        "# X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB6zvYgJan4Z"
      },
      "source": [
        "## 6) 建立網路模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJvDPRx5an4Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q30p2WHxan4d"
      },
      "source": [
        "# 此範例使用 Tensorflow2.0 Sequential API 搭建神經網路。\n",
        "model = Sequential()\n",
        "model.add(Dense(8, Activation('relu'), input_dim=X.shape[-1]))\n",
        "model.add(Dense(16, Activation('relu')))\n",
        "model.add(Dense(205, Activation('softmax')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCfdh7Lgan4h"
      },
      "source": [
        "model.summary()   # Weights = (4+1)*8+(8+1)*16+(16+1)*3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86S7ALLTan4l"
      },
      "source": [
        "# 編譯模型\n",
        "optim = Adam(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optim,\n",
        "              metrics=['acc'])\n",
        "\n",
        "batch_size=1000\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    shuffle=True,\n",
        "                    validation_split=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG1O5ymTan4s"
      },
      "source": [
        "## 7) 觀察訓練結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFUTBafCan4u"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "epochs_ = range(1,len(acc)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5PL5GAwan4x"
      },
      "source": [
        "plt.plot(epochs_ , loss , label = 'training loss')\n",
        "plt.plot(epochs_ , val_loss , label = 'val los')\n",
        "plt.title('training and val loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ketuv_Bvan43"
      },
      "source": [
        "plt.clf()\n",
        "plt.plot(epochs_ , acc , label='train accuracy')\n",
        "plt.plot(epochs_ , val_acc , label = 'val accuracy')\n",
        "plt.title('train and val acc')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR5QIun7an47"
      },
      "source": [
        "## 8) 測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdG9q4M9an47"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "label=np.argmax(y,axis=1)\n",
        "pred =  np.argmax(model.predict(X), axis=1)\n",
        "# print(pred.shape)\n",
        "print(accuracy_score(label, pred))\n",
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNWE7Fe8uBzn"
      },
      "source": [
        "!pip install eli5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNxXer-qOlwj"
      },
      "source": [
        "## 9) 檢查特徵重要性\n",
        "> https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html\n",
        "> https://www.mdeditor.tw/pl/pgi5/zh-tw\n",
        "\n",
        "> 針對BlackBox模型或非sklearn模型，使用排列重要性方法\n",
        "\n",
        "\n",
        "> 捨棄重要性低、增加損失的features特徵，重新建模訓練。\n",
        "\n",
        "\n",
        "> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-t6pvZman4_"
      },
      "source": [
        "# import numpy as np\n",
        "from eli5.permutation_importance import get_score_importances\n",
        "\n",
        "# ... load data, define score function\n",
        "def score(X, y):\n",
        "    y_pred = np.argmax(model.predict(X), axis=1)\n",
        "    return accuracy_score(y, y_pred)\n",
        "\n",
        "lab=np.argmax(y_train,axis=1)\n",
        "base_score, score_decreases = get_score_importances(score, X_train.values, lab)\n",
        "feature_importances = np.mean(score_decreases, axis=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz2cPz1KQtQy"
      },
      "source": [
        "# print(feature_importances)\n",
        "feature_importance_dict = {}\n",
        "for i, feature_name in enumerate(X.columns.values.tolist()) :\n",
        "\tfeature_importance_dict[feature_name]=feature_importances[i]\n",
        "\n",
        "# print(dict(sorted(feature_importance_dict.items(), key=lambda x:x[1],reverse=True)[:5]))\n",
        "sorted_fi = dict(sorted(feature_importance_dict.items(), key=lambda x:x[1],reverse=True))\n",
        "for name,i in sorted_fi.items():\n",
        "  print(name, i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}